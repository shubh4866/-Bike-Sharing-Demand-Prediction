{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d834ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84733aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Bike_Sharing_Demand_Prediction_Capstone_Project(Shubhashis).ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1dsj7Gp61EvI0xCfiD2PIlrglNNYQlFlX\n",
    "\n",
    "#<font color='red'> <b> Project : Bike Sharing Demand Prediction </b>\n",
    "\n",
    "## <font color='Green'> <b>  Problem Description </b>\n",
    "\n",
    "### Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.\n",
    "\n",
    "## <font color='green'><b> Data Description </b>\n",
    "\n",
    "### <b> The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.</b>\n",
    "\n",
    "\n",
    "### <b>Attribute Information: </b>\n",
    "\n",
    "* ### Date : year-month-day\n",
    "* ### Rented Bike count - Count of bikes rented at each hour\n",
    "* ### Hour - Hour of he day\n",
    "* ### Temperature-Temperature in Celsius\n",
    "* ### Humidity - %\n",
    "* ### Windspeed - m/s\n",
    "* ### Visibility - 10m\n",
    "* ### Dew point temperature - Celsius\n",
    "* ### Solar radiation - MJ/m2\n",
    "* ### Rainfall - mm\n",
    "* ### Snowfall - cm\n",
    "* ### Seasons - Winter, Spring, Summer, Autumn\n",
    "* ### Holiday - Holiday/No holiday\n",
    "* ### Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)\n",
    "\n",
    "# <font color='Green'>**Loading Dataset and Importing Modules**\n",
    "\"\"\"\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "#importing the modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\"##<font color='Green'>importing the datset\"\"\"\n",
    "\n",
    "#mounting google drive for import the dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#loading the dataset\n",
    "df=pd.read_csv('/content/drive/MyDrive/1.Project/Bike Sharing Demand Prediction/SeoulBikeData.csv',encoding ='latin')\n",
    "\n",
    "\"\"\"\n",
    "# <font color='Green'>**Getting insight from data**\"\"\"\n",
    "\n",
    "# top 5 rows\n",
    "df.head()\n",
    "\n",
    "#shape of dataset\n",
    "print(df.shape)\n",
    "\n",
    "# Data set columns\n",
    "df.columns\n",
    "\n",
    "#info of data set\n",
    "df.info()\n",
    "\n",
    "# the unique value for the dataset\n",
    "df.nunique()\n",
    "\n",
    "\"\"\"##<font color='blue'>Features description\n",
    "\n",
    "**Date** : *The date of the day, during 365 days from 01/12/2017 to 30/11/2018, formating in DD/MM/YYYY, type : str*, we need to convert into datetime format.\n",
    "\n",
    "**Rented Bike Count** : *Number of rented bikes per hour which our dependent variable and we need to predict that, type : int*\n",
    "\n",
    "**Hour**: *The hour of the day, starting from 0-23 it's in a digital time format, type : int, we need to convert it into category data type.*\n",
    "\n",
    "**Temperature(째C)**: *Temperature in Celsius, type : Float*\n",
    "\n",
    "**Humidity(%)**: *Humidity in the air in %, type : int*\n",
    "\n",
    "**Wind speed (m/s)** : *Speed of the wind in m/s, type : Float*\n",
    "\n",
    "**Visibility (10m)**: *Visibility in m, type : int*\n",
    "\n",
    "**Dew point temperature(째C)**: *Temperature at the beggining of the day, type : Float*\n",
    "\n",
    "**Solar Radiation (MJ/m2)**: *Sun contribution, type : Float*\n",
    "\n",
    "**Rainfall(mm)**: *Amount of raining in mm, type : Float*\n",
    "\n",
    "**Snowfall (cm)**: *Amount of snowing in cm, type : Float*\n",
    "\n",
    "**Seasons**: *Season of the year, type : str, there are only 4 season's in data *. \n",
    "\n",
    "**Holiday**: *If the day  is holiday period or not, type: str*\n",
    "\n",
    "**Functioning Day**: *If the day is a Functioning Day or not, type : str*\n",
    "\n",
    "# <font color='blue'>**Preprocessing the dataset**\n",
    "\n",
    "##<font color='green'>Missing values\n",
    "\"\"\"\n",
    "\n",
    "#check for count of missing values in each column.\n",
    "df.isnull().sum()\n",
    "\n",
    "\"\"\"##<font color='green'>Duplicate values\"\"\"\n",
    "\n",
    "# Checking Duplicate Values\n",
    "len(df[df.duplicated()])\n",
    "\n",
    "\"\"\"##<font color='blue'>changing column name\"\"\"\n",
    "\n",
    "#Rename the complex columns name\n",
    "df=df.rename(columns={'Rented Bike Count':'Rented_Bike_Count',\n",
    "                                'Temperature(째C)':'Temperature',\n",
    "                                'Humidity(%)':'Humidity',\n",
    "                                'Wind speed (m/s)':'Wind_speed',\n",
    "                                'Visibility (10m)':'Visibility',\n",
    "                                'Dew point temperature(째C)':'Dew_point_temperature',\n",
    "                                'Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
    "                                'Rainfall(mm)':'Rainfall',\n",
    "                                'Snowfall (cm)':'Snowfall',\n",
    "                                'Functioning Day':'Functioning_Day'})\n",
    "\n",
    "\"\"\"##<font color='orange'> Extracting ( year,month,day) from date colums\"\"\"\n",
    "\n",
    "# Changing the \"Date\" column into three \"year\",\"month\",\"day\" column\n",
    "df['Date'] = df['Date'].apply(lambda x:dt.datetime.strptime(x,\"%d/%m/%Y\"))\n",
    "\n",
    "df['year'] = df['Date'].dt.year\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day'] = df['Date'].dt.day_name()\n",
    "\n",
    "#creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
    "df['weekdays_weekend']=df['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
    "df=df.drop(columns=['Date','day','year'],axis=1)\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "\n",
    "df['weekdays_weekend'].value_counts()\n",
    "\n",
    "\"\"\"*<font color='red'> ***As \"Hour\",\"month\",\"weekdays_weekend\" column are show as a integer data type but actually it is a category data type , so we need to change this data type.While performing further analysis and correleted with these column the values are not actually true so we can mislead by this.***\"\"\"\n",
    "\n",
    "#changing integer to categorical values\n",
    "df[['Hour','month','weekdays_weekend']].apply(lambda x: x.astype('category'))\n",
    "df.info()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# <font color='red'> **Exploratory Data Analysis**\n",
    "\n",
    "<font color='Green'> **From the below bar plot we can clearly say that from  the month 5 to 10 the demand of the rented bike is high as compare to other months.These are summer season months.**\n",
    "\"\"\"\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(20,8))\n",
    "sns.barplot(data=df,x='month',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
    "ax.set(title='Count of Rented bikes acording to Month ')\n",
    "\n",
    "\"\"\"####<font color='blue'> **weekdays vs weekend**\"\"\"\n",
    "\n",
    "#anlysis of data by vizualisation\n",
    "fig,ax=plt.subplots(figsize=(10,8))\n",
    "sns.barplot(data=df,x='weekdays_weekend',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
    "ax.set(title='Count of Rented bikes acording to weekdays_weekenday ')\n",
    "\n",
    "#anlysis of data by vizualisation\n",
    "fig,ax=plt.subplots(figsize=(20,8))\n",
    "sns.pointplot(data=df,x='Hour',y='Rented_Bike_Count',hue='weekdays_weekend',ax=ax)\n",
    "ax.set(title='Count of Rented bikes acording to weekdays_weekend ')\n",
    "\n",
    "\"\"\"<font color='Blue'>***From the above point plot and bar plot we can say that in the week days which represent in blue colur shows that the demand of the bike higher because of the office.***\n",
    "\n",
    "<font color='Blue'>***Peak Time are 7 am to 9 am and 5 pm to 7 pm***\n",
    "\n",
    "<font color='Blue'>***The orange colur represent the weekend days, and it show that the demand of rented bikes are very low specially in the morning hour but when the evening start from 4 pm to 8 pm the demand slightly increases.***   \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#anlysis of data by vizualisation\n",
    "fig,ax=plt.subplots(figsize=(20,8))\n",
    "sns.barplot(data=df,x='Hour',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
    "ax.set(title='Count of Rented bikes acording to Hour ')\n",
    "\n",
    "\"\"\"\n",
    "<font color='blue'> *** people use rented bikes during their working hour from 7am to 9am and 5pm to 7pm.**\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#anlysis of data by vizualisation\n",
    "fig,ax=plt.subplots(figsize=(20,8))\n",
    "sns.pointplot(data=df,x='Hour',y='Rented_Bike_Count',hue='Seasons',ax=ax)\n",
    "ax.set(title='Count of Rented bikes acording to seasons ')\n",
    "\n",
    "\"\"\"*<font color='gree'> In the above point plot which shows the use of rented bike in in four different seasons, and it clearly shows that in summer season the use of rented bike is high and peak time is 7am-9am and 7pm-5pm.\n",
    "\n",
    "*<font color='gree'> In winter season the use of rented bike is very low because of snowfall.\n",
    "\"\"\"\n",
    "\n",
    "#anlysis of data by vizualisation\n",
    "fig,ax=plt.subplots(figsize=(20,8))\n",
    "sns.pointplot(data=df,x='Hour',y='Rented_Bike_Count',hue='Holiday',ax=ax)\n",
    "ax.set(title='Count of Rented bikes acording to Holiday ')\n",
    "\n",
    "\"\"\"*<font color = green > ***In the above point plot which shows the use of rented bike in a holiday, and it clearly shows that in holiday people uses the rented bike from 2pm-8pm***\"\"\"\n",
    "\n",
    "## Bike Rent Count trend with respect Hours on Months\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.pointplot(x=df['Hour'],y=df['Rented_Bike_Count'],hue=df['month'])\n",
    "plt.title(\"Bike Rental Trend according to Hour in different months\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"##<font color='red'> **Analyze  of Numerical variables**\"\"\"\n",
    "\n",
    "#assign the numerical coulmn to variable\n",
    "numerical_columns=list(df.select_dtypes(['int64','float64']).columns)\n",
    "numerical_features=pd.Index(numerical_columns)\n",
    "numerical_features\n",
    "\n",
    "# displots to analyze the distribution of all numerical features\n",
    "for col in numerical_features:\n",
    "  plt.figure(figsize=(10,6))\n",
    "  sns.distplot(x=df[col])\n",
    "  plt.xlabel(col)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"#### <font color='blue'>Numerical vs.Rented_Bike_Count\"\"\"\n",
    "\n",
    "#the plot show analyze the relationship between \"Rented_Bike_Count\" and \"Temperature\" \n",
    "df.groupby('Temperature').mean()['Rented_Bike_Count'].plot()\n",
    "\n",
    "\"\"\"* ***From the above plot we can see that people like to ride bikes when it is pretty hot around 25째C in average***\"\"\"\n",
    "\n",
    "# plot to analyze the relationship between \"Rented_Bike_Count\" and \"Dew_point_temperature\" \n",
    "df.groupby('Dew_point_temperature').mean()['Rented_Bike_Count'].plot()\n",
    "\n",
    "\"\"\"* ***From the above plot of \"Dew_point_temperature' is almost same as the 'temperature' there is some similarity present we can check it in our next step.***\"\"\"\n",
    "\n",
    "#plot to analyze the relationship between \"Rented_Bike_Count\" and \"Solar_Radiation\" \n",
    "df.groupby('Solar_Radiation').mean()['Rented_Bike_Count'].plot()\n",
    "\n",
    "\"\"\"* ***from the above plot we see that, the amount of rented bikes is huge, when there is solar radiation, the counter of rents is around 1000***\"\"\"\n",
    "\n",
    "#plot to analyze the relationship between \"Rented_Bike_Count\" and \"Snowfall\" \n",
    "df.groupby('Snowfall').mean()['Rented_Bike_Count'].plot()\n",
    "\n",
    "\"\"\"* ***We can see from the plot that, on the y-axis, the amount of rented bike is very low When we have more than 4 cm of snow, the bike rents is much lower***\"\"\"\n",
    "\n",
    "#plot to analyze the relationship between \"Rented_Bike_Count\" and \"Rainfall\" \n",
    "df.groupby('Rainfall').mean()['Rented_Bike_Count'].plot()\n",
    "\n",
    "\"\"\"* ***We can see from the above plot that even if it rains a lot the demand of of rent bikes is not decreasing, here for example even if we have 20 mm of rain there is a big peak of rented bikes***\"\"\"\n",
    "\n",
    "#plot to analyze the relationship between \"Rented_Bike_Count\" and \"Wind_speed\" \n",
    "df.groupby('Wind_speed').mean()['Rented_Bike_Count'].plot()\n",
    "\n",
    "\"\"\"* ***We can see from the above plot that the demand of rented bike is uniformly distribute despite of wind speed but when the speed of wind was 7 m/s then the demand of bike also increase that clearly means peoples love to ride bikes when its little windy.***\n",
    "\n",
    "##<B><font color= blue> Regression plot\n",
    "\"\"\"\n",
    "\n",
    "#regression plot for all the numerical features\n",
    "for col in numerical_features:\n",
    "  fig,ax=plt.subplots(figsize=(10,6))\n",
    "  sns.regplot(x=df[col],y=df['Rented_Bike_Count'],scatter_kws={\"color\": 'lightblue'}, line_kws={\"color\": \"red\"})\n",
    "\n",
    "\"\"\"* ***From the above regression plot of all numerical features we see that the columns  'Temperature', 'Wind_speed','Visibility', 'Dew_point_temperature', 'Solar_Radiation' are positively relation to the target variable.***\n",
    "\n",
    "\n",
    "* ***which means the rented bike count increases with increase of these features.***\n",
    "* ***'Rainfall','Snowfall','Humidity' these features are negatively related with the target variaable which means the rented bike count decreases when these features increase.***\n",
    "\n",
    "## <font color='red'>**Normalise Rented_Bike_Count column data**\n",
    "\"\"\"\n",
    "\n",
    "#Distribution plot of Rented Bike Count\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xlabel('Rented_Bike_Count')\n",
    "plt.ylabel('Density')\n",
    "ax=sns.distplot(df['Rented_Bike_Count'],hist=True ,color=\"b\")\n",
    "ax.axvline(df['Rented_Bike_Count'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "ax.axvline(df['Rented_Bike_Count'].median(), color='black', linestyle='dashed', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"* ***The above graph shows that Rented Bike Count has moderate right skewness. Since the assumption of linear regression is that 'the distribution of dependent variable has to be normal', so we should perform some operation to make it normal.***\"\"\"\n",
    "\n",
    "#Boxplot of Rented Bike Count to check outliers\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.ylabel('Rented_Bike_Count')\n",
    "sns.boxplot(x=df['Rented_Bike_Count'])\n",
    "plt.show()\n",
    "\n",
    "\"\"\"* ***The above boxplot shows that we have detect outliers in Rented Bike Count column***\"\"\"\n",
    "\n",
    "#Applying square root to Rented Bike Count to improve skewness\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.xlabel('Rented Bike Count')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "ax=sns.distplot(np.sqrt(df['Rented_Bike_Count']), color=\"g\")\n",
    "ax.axvline(np.sqrt(df['Rented_Bike_Count']).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "ax.axvline(np.sqrt(df['Rented_Bike_Count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"* ***Since we have generic rule of applying Square root for the skewed variable in order to make it normal .After applying Square root to the skewed Rented Bike Count, here we get almost normal distribution.***\"\"\"\n",
    "\n",
    "#After applying sqrt on Rented Bike Count to check wheater we still have outliers \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.ylabel('Rented_Bike_Count')\n",
    "sns.boxplot(x=np.sqrt(df['Rented_Bike_Count']))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"## <font color='green'>**Checking of Correlation between variables**\n",
    "\n",
    "### <font color='blue'>Checking in OLS Model\n",
    "\n",
    "**Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable**\n",
    "\"\"\"\n",
    "\n",
    "#import the module\n",
    "#assign the 'x','y' value\n",
    "import statsmodels.api as sm\n",
    "X = df[[ 'Temperature','Humidity',\n",
    "       'Wind_speed', 'Visibility','Dew_point_temperature',\n",
    "       'Solar_Radiation', 'Rainfall', 'Snowfall']]\n",
    "Y = df['Rented_Bike_Count']\n",
    "df.head()\n",
    "\n",
    "#add a constant column\n",
    "X = sm.add_constant(X)\n",
    "X\n",
    "\n",
    "## fit a OLS model \n",
    "\n",
    "model= sm.OLS(Y, X).fit()\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "* **R square and Adj Square are near to each other. 40% of variance in the Rented Bike count is  explained by the model.**\n",
    "*  **For F statistic , P value is less than 0.05 for 5% levelof significance.**\n",
    "*  **P value of dew point temp and visibility are very high and they are not significant.**\n",
    "\n",
    "*  **Omnibus tests the skewness and kurtosis of the residuals. Here the value of Omnibus is high., it shows we have skewness in our data.**\n",
    "*  **The condition number is large, 3.11e+04. This might indicate that there are strong multicollinearity or other numerical problems**   \n",
    "*  **Durbin-Watson tests for autocorrelation of the residuals. Here value is less than 0.5. We can say that there exists a positive auto correlation among the variables.**\"\"\"\n",
    "\n",
    "X.corr()\n",
    "\n",
    "\"\"\"* ***From the OLS model we find that the 'Temperature' and  'Dew_point_temperature' are highly correlated so we need to drop one of them.***\n",
    "* ***for droping the we check the (P>|t|) value from above table and we can see that the 'Dew_point_temperature' value is higher so we need to drop Dew_point_temperature column***\n",
    "* ***For clarity, we use visualisation i.e heatmap in next step***\n",
    "\n",
    "### <font color='red'>**Heatmap**\n",
    "\n",
    "* **we check correletion betweeen variables using Correlation heatmap, it is graphical representation of correlation matrix representing correlation between different variables**\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## plot the Correlation matrix\n",
    "plt.figure(figsize=(20,8))\n",
    "correlation=df.corr()\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "sns.heatmap((correlation),mask=mask, annot=True,cmap='coolwarm')\n",
    "\n",
    "\"\"\"***We can observe on the heatmap that on the target variable line the most positively correlated variables to the rent are :***\n",
    "\n",
    "* the temperature\n",
    "* the dew point temperature\n",
    "* the solar radiation\n",
    "\n",
    "***And most negatively correlated variables are:***\n",
    "* Humidity\n",
    "* Rainfall\n",
    "\n",
    "* ***From the above correlation heatmap, We see that there is a positive \n",
    "correlation between columns 'Temperature' and 'Dew point temperature' i.e 0.91 so even if we drop this column then it dont affects the outcome of our analysis. And they have the same variations.. so we can drop the column 'Dew point temperature(째C)'.***\n",
    "\"\"\"\n",
    "\n",
    "#drop the Dew point temperature column\n",
    "df=df.drop(['Dew_point_temperature'],axis=1)\n",
    "\n",
    "df.info()\n",
    "\n",
    "\"\"\"##  <font color='orange'>Create the dummy variables \"\"\"\n",
    "\n",
    "#Assign all catagoriacla features to a variable\n",
    "categorical_features=list(df.select_dtypes(['object','category']).columns)\n",
    "categorical_features=pd.Index(categorical_features)\n",
    "categorical_features\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "###<font color='blue'>one hot encoding\"\"\"\n",
    "\n",
    "#creat a copy\n",
    "df_copy = df\n",
    "\n",
    "def one_hot_encoding(data, column):\n",
    "    data = pd.concat([data, pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)\n",
    "    data = data.drop([column], axis=1)\n",
    "    return data\n",
    "\n",
    "for col in categorical_features:\n",
    "    df_copy = one_hot_encoding(df_copy, col)\n",
    "df_copy.head()\n",
    "\n",
    "\"\"\"# <font color='RED'>**Model Training**\n",
    "\n",
    "## <font color='GREEN'>**Train Test split for regression**\n",
    "\"\"\"\n",
    "\n",
    "#Assign the value in X and Y\n",
    "X = df_copy.drop(columns=['Rented_Bike_Count'], axis=1)\n",
    "y = np.sqrt(df_copy['Rented_Bike_Count'])\n",
    "\n",
    "X.head()\n",
    "\n",
    "y.head()\n",
    "\n",
    "#Creat test and train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=0)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "df_copy.describe().columns\n",
    "\n",
    "\"\"\"#  <font color='reD'>**LINEAR REGRESSION**\"\"\"\n",
    "\n",
    "#import the packages\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg= LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "#check the score\n",
    "reg.score(X_train, y_train)\n",
    "\n",
    "#check the coefficeint\n",
    "reg.coef_\n",
    "\n",
    "#get the X_train and X-test value\n",
    "y_pred_train=reg.predict(X_train)\n",
    "y_pred_test=reg.predict(X_test)\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#calculate MSE\n",
    "MSE_LR= mean_squared_error((y_train), (y_pred_train))\n",
    "print(\"MSE :\",MSE_LR)\n",
    "\n",
    "#calculate RMSE\n",
    "RMSE_LR=np.sqrt(MSE_LR)\n",
    "print(\"RMSE :\",RMSE_LR)\n",
    "\n",
    "\n",
    "#calculate MAE\n",
    "MAE_LR= mean_absolute_error(y_train, y_pred_train)\n",
    "print(\"MAE :\",MAE_LR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_LR= r2_score(y_train, y_pred_train)\n",
    "print(\"R2 :\",r2_LR)\n",
    "Adjusted_R2_LR = (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**Looks like our r2 score value is 0.65 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict1={'Model':'Linear regression ',\n",
    "       'MAE':round((MAE_LR),3),\n",
    "       'MSE':round((MSE_LR),3),\n",
    "       'RMSE':round((RMSE_LR),3),\n",
    "       'R2_score':round((r2_LR),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_LR ),2)\n",
    "       }\n",
    "training_df=pd.DataFrame(dict1,index=[1])\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#calculate MSE\n",
    "MSE_LR= mean_squared_error(y_test, y_pred_test)\n",
    "print(\"MSE :\",MSE_LR)\n",
    "\n",
    "#calculate RMSE\n",
    "RMSE_LR=np.sqrt(MSE_LR)\n",
    "print(\"RMSE :\",RMSE_LR)\n",
    "\n",
    "\n",
    "#calculate MAE\n",
    "MAE_LR= mean_absolute_error(y_test, y_pred_test)\n",
    "print(\"MAE :\",MAE_LR)\n",
    "\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_LR= r2_score((y_test), (y_pred_test))\n",
    "print(\"R2 :\",r2_LR)\n",
    "Adjusted_R2_LR = (1-(1-r2_score((y_test), (y_pred_test)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
    "print(\"Adjusted R2 :\",Adjusted_R2_LR )\n",
    "\n",
    "\"\"\"**The r2_score for the test set is 0.66. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict2={'Model':'Linear regression ',\n",
    "       'MAE':round((MAE_LR),3),\n",
    "       'MSE':round((MSE_LR),3),\n",
    "       'RMSE':round((RMSE_LR),3),\n",
    "       'R2_score':round((r2_LR),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_LR ),2)\n",
    "       }\n",
    "test_df=pd.DataFrame(dict2,index=[1])\n",
    "\n",
    "### Heteroscadacity\n",
    "plt.scatter((y_pred_test),(y_test)-(y_pred_test))\n",
    "\n",
    "#Plot the figure\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(y_pred_test)\n",
    "plt.plot(np.array(y_test))\n",
    "plt.legend([\"Predicted\",\"Actual\"])\n",
    "plt.xlabel('No of Test Data')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"#<font color='reD'>**LASSO REGRESSION** \"\"\"\n",
    "\n",
    "# Create an instance of Lasso Regression implementation\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=1.0, max_iter=3000)\n",
    "# Fit the Lasso model\n",
    "lasso.fit(X_train, y_train)\n",
    "# Create the model score\n",
    "print(lasso.score(X_test, y_test), lasso.score(X_train, y_train))\n",
    "\n",
    "#get the X_train and X-test value\n",
    "y_pred_train_lasso=lasso.predict(X_train)\n",
    "y_pred_test_lasso=lasso.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#calculate MSE\n",
    "MSE_l= mean_squared_error((y_train), (y_pred_train_lasso))\n",
    "print(\"MSE :\",MSE_l)\n",
    "\n",
    "#calculate RMSE\n",
    "RMSE_l=np.sqrt(MSE_l)\n",
    "print(\"RMSE :\",RMSE_l)\n",
    "\n",
    "\n",
    "#calculate MAE\n",
    "MAE_l= mean_absolute_error(y_train, y_pred_train_lasso)\n",
    "print(\"MAE :\",MAE_l)\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_l= r2_score(y_train, y_pred_train_lasso)\n",
    "print(\"R2 :\",r2_l)\n",
    "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**Looks like our r2 score value is 0.47 that means our model is not able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict1={'Model':'Lasso regression ',\n",
    "       'MAE':round((MAE_l),3),\n",
    "       'MSE':round((MSE_l),3),\n",
    "       'RMSE':round((RMSE_l),3),\n",
    "       'R2_score':round((r2_l),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_l ),2)\n",
    "       }\n",
    "training_df=training_df.append(dict1,ignore_index=True)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#calculate MSE\n",
    "MSE_l= mean_squared_error(y_test, y_pred_test_lasso)\n",
    "print(\"MSE :\",MSE_l)\n",
    "\n",
    "#calculate RMSE\n",
    "RMSE_l=np.sqrt(MSE_l)\n",
    "print(\"RMSE :\",RMSE_l)\n",
    "\n",
    "\n",
    "#calculate MAE\n",
    "MAE_l= mean_absolute_error(y_test, y_pred_test_lasso)\n",
    "print(\"MAE :\",MAE_l)\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_l= r2_score((y_test), (y_pred_test_lasso))\n",
    "print(\"R2 :\",r2_l)\n",
    "Adjusted_R2_l=(1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**The r2_score for the test set is 0.45. This means our linear model is  not performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict2={'Model':'Lasso regression ',\n",
    "       'MAE':round((MAE_l),3),\n",
    "       'MSE':round((MSE_l),3),\n",
    "       'RMSE':round((RMSE_l),3),\n",
    "       'R2_score':round((r2_l),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_l ),2),\n",
    "       }\n",
    "test_df=test_df.append(dict2,ignore_index=True)\n",
    "\n",
    "#Plot the figure\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(np.array(y_pred_test_lasso))\n",
    "plt.plot(np.array((y_test)))\n",
    "plt.legend([\"Predicted\",\"Actual\"])\n",
    "plt.show()\n",
    "\n",
    "### Heteroscadacity\n",
    "plt.scatter((y_pred_test_lasso),(y_test-y_pred_test_lasso))\n",
    "\n",
    "\"\"\"##<font color =red> **RIDGE REGRESSION**\"\"\"\n",
    "\n",
    "#import the packages\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge= Ridge(alpha=0.1)\n",
    "\n",
    "#Fit the model\n",
    "ridge.fit(X_train,y_train)\n",
    "\n",
    "#check the score\n",
    "ridge.score(X_train, y_train)\n",
    "\n",
    "#get the X_train and X-test value\n",
    "y_pred_train_ridge=ridge.predict(X_train)\n",
    "y_pred_test_ridge=ridge.predict(X_test)\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#calculate MSE\n",
    "MSE_r= mean_squared_error((y_train), (y_pred_train_ridge))\n",
    "print(\"MSE :\",MSE_r)\n",
    "#calculate RMSE\n",
    "RMSE_r=np.sqrt(MSE_r)\n",
    "print(\"RMSE :\",RMSE_r)\n",
    "#calculate MAE\n",
    "MAE_r= mean_absolute_error(y_train, y_pred_train_ridge)\n",
    "print(\"MAE :\",MAE_r)\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_r= r2_score(y_train, y_pred_train_ridge)\n",
    "print(\"R2 :\",r2_r)\n",
    "Adjusted_R2_r=(1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**Looks like our r2 score value is 0.65 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict1={'Model':'Ridge regression ',\n",
    "       'MAE':round((MAE_r),3),\n",
    "       'MSE':round((MSE_r),3),\n",
    "       'RMSE':round((RMSE_r),3),\n",
    "       'R2_score':round((r2_r),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
    "training_df=training_df.append(dict1,ignore_index=True)\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#calculate MSE\n",
    "MSE_r= mean_squared_error(y_test, y_pred_test_ridge)\n",
    "print(\"MSE :\",MSE_r)\n",
    "#calculate RMSE\n",
    "RMSE_r=np.sqrt(MSE_r)\n",
    "print(\"RMSE :\",RMSE_r)\n",
    "#calculate MAE\n",
    "MAE_r= mean_absolute_error(y_test, y_pred_test_ridge)\n",
    "print(\"MAE :\",MAE_r)\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_r= r2_score((y_test), (y_pred_test_ridge))\n",
    "print(\"R2 :\",r2_r)\n",
    "Adjusted_R2_r=(1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**The r2_score for the test set is 0.66. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict2={'Model':'Ridge regression ',\n",
    "       'MAE':round((MAE_r),3),\n",
    "       'MSE':round((MSE_r),3),\n",
    "       'RMSE':round((RMSE_r),3),\n",
    "       'R2_score':round((r2_r),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
    "test_df=test_df.append(dict2,ignore_index=True)\n",
    "\n",
    "#Plot the figure\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot((y_pred_test_ridge))\n",
    "plt.plot((np.array(y_test)))\n",
    "plt.legend([\"Predicted\",\"Actual\"])\n",
    "plt.show()\n",
    "\n",
    "### Heteroscadacity\n",
    "plt.scatter((y_pred_test_ridge),(y_test)-(y_pred_test_ridge))\n",
    "\n",
    "\"\"\"# <font color = red > **ELASTIC NET REGRESSION**\"\"\"\n",
    "\n",
    "#import the packages\n",
    "from sklearn.linear_model import ElasticNet\n",
    "#a * L1 + b * L2\n",
    "#alpha = a + b and l1_ratio = a / (a + b)\n",
    "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "#fit the model \n",
    "elasticnet.fit(X_train,y_train)\n",
    "\n",
    "#check the score\n",
    "elasticnet.score(X_train, y_train)\n",
    "\n",
    "#get the X_train and X-test value\n",
    "y_pred_train_en=elasticnet.predict(X_train)\n",
    "y_pred_test_en=elasticnet.predict(X_test)\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#calculate MSE\n",
    "MSE_e= mean_squared_error((y_train), (y_pred_train_en))\n",
    "print(\"MSE :\",MSE_e)\n",
    "#calculate RMSE\n",
    "RMSE_e=np.sqrt(MSE_e)\n",
    "print(\"RMSE :\",RMSE_e)\n",
    "#calculate MAE\n",
    "MAE_e= mean_absolute_error(y_train, y_pred_train_en)\n",
    "print(\"MAE :\",MAE_e)\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_e= r2_score(y_train, y_pred_train_en)\n",
    "print(\"R2 :\",r2_e)\n",
    "Adjusted_R2_e=(1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**Looks like our r2 score value is 0.58 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict1={'Model':'Elastic net regression ',\n",
    "       'MAE':round((MAE_e),3),\n",
    "       'MSE':round((MSE_e),3),\n",
    "       'RMSE':round((RMSE_e),3),\n",
    "       'R2_score':round((r2_e),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
    "training_df=training_df.append(dict1,ignore_index=True)\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#calculate MSE\n",
    "MSE_e= mean_squared_error(y_test, y_pred_test_en)\n",
    "print(\"MSE :\",MSE_e)\n",
    "#calculate RMSE\n",
    "RMSE_e=np.sqrt(MSE_e)\n",
    "print(\"RMSE :\",RMSE_e)\n",
    "#calculate MAE\n",
    "MAE_e= mean_absolute_error(y_test, y_pred_test_en)\n",
    "print(\"MAE :\",MAE_e)\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_e= r2_score((y_test), (y_pred_test_en))\n",
    "print(\"R2 :\",r2_e)\n",
    "Adjusted_R2_e=(1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**The r2_score for the test set is 0.57. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict2={'Model':'Elastic net regression Test',\n",
    "       'MAE':round((MAE_e),3),\n",
    "       'MSE':round((MSE_e),3),\n",
    "       'RMSE':round((RMSE_e),3),\n",
    "       'R2_score':round((r2_e),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
    "test_df=test_df.append(dict2,ignore_index=True)\n",
    "\n",
    "#Plot the figure\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(np.array(y_pred_test_en))\n",
    "plt.plot((np.array(y_test)))\n",
    "plt.legend([\"Predicted\",\"Actual\"])\n",
    "plt.show()\n",
    "\n",
    "### Heteroscadacity\n",
    "plt.scatter((y_pred_test_en),(y_test)-(y_pred_test_en))\n",
    "\n",
    "\"\"\"#<font color = red> **DECISION TREE**\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#import the packages\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "decision_regressor = DecisionTreeRegressor(criterion='mse', max_depth=8,\n",
    "                      max_features=9, max_leaf_nodes=100,)\n",
    "decision_regressor.fit(X_train, y_train)\n",
    "\n",
    "#get the X_train and X-test value\n",
    "y_pred_train_d = decision_regressor.predict(X_train)\n",
    "y_pred_test_d = decision_regressor.predict(X_test)\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Model Score:\",decision_regressor.score(X_train,y_train))\n",
    "#calculate MSE\n",
    "MSE_d= mean_squared_error(y_train, y_pred_train_d)\n",
    "print(\"MSE :\",MSE_d)\n",
    "#calculate RMSE\n",
    "RMSE_d=np.sqrt(MSE_d)\n",
    "print(\"RMSE :\",RMSE_d)\n",
    "#calculate MAE\n",
    "MAE_d= mean_absolute_error(y_train, y_pred_train_d)\n",
    "print(\"MAE :\",MAE_d)\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_d= r2_score(y_train, y_pred_train_d)\n",
    "print(\"R2 :\",r2_d)\n",
    "Adjusted_R2_d=(1-(1-r2_score(y_train, y_pred_train_d))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_d))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**Looks like our r2 score value is 0.85 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict1={'Model':'Dicision tree regression ',\n",
    "       'MAE':round((MAE_d),3),\n",
    "       'MSE':round((MSE_d),3),\n",
    "       'RMSE':round((RMSE_d),3),\n",
    "       'R2_score':round((r2_d),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_d),2)\n",
    "      }\n",
    "training_df=training_df.append(dict1,ignore_index=True)\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#calculate MSE\n",
    "MSE_d= mean_squared_error(y_test, y_pred_test_d)\n",
    "print(\"MSE :\",MSE_d)\n",
    "#calculate RMSE\n",
    "RMSE_d=np.sqrt(MSE_d)\n",
    "print(\"RMSE :\",RMSE_d)\n",
    "#calculate MAE\n",
    "MAE_d= mean_absolute_error(y_test, y_pred_test_d)\n",
    "print(\"MAE :\",MAE_d)\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_d= r2_score((y_test), (y_pred_test_d))\n",
    "print(\"R2 :\",r2_d)\n",
    "Adjusted_R2_d=(1-(1-r2_score((y_test), (y_pred_test_d)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_d)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**The r2_score for the test set is 0.81. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict2={'Model':'Dicision tree regression ',\n",
    "       'MAE':round((MAE_d),3),\n",
    "       'MSE':round((MSE_d),3),\n",
    "       'RMSE':round((RMSE_d),3),\n",
    "       'R2_score':round((r2_d),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_d),2)\n",
    "      }\n",
    "test_df=test_df.append(dict2,ignore_index=True)\n",
    "\n",
    "#Plot the figure\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot((np.array(y_pred_test_d)))\n",
    "plt.plot(np.array((y_test)))\n",
    "plt.legend([\"Predicted\",\"Actual\"])\n",
    "plt.show()\n",
    "\n",
    "### Heteroscadacity\n",
    "plt.scatter((y_pred_test_d),(y_test)-(y_pred_test_d))\n",
    "\n",
    "\"\"\"#<font color =red> **RANDOM FOREST**\"\"\"\n",
    "\n",
    "#import the packages\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Create an instance of the RandomForestRegressor\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train,y_train)\n",
    "\n",
    "# Making predictions on train and test data\n",
    "y_pred_train_r = rf_model.predict(X_train)\n",
    "y_pred_test_r = rf_model.predict(X_test)\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Model Score:\",rf_model.score(X_train,y_train))\n",
    "#calculate MSE\n",
    "MSE_rf= mean_squared_error(y_train, y_pred_train_r)\n",
    "print(\"MSE :\",MSE_rf)\n",
    "#calculate RMSE\n",
    "RMSE_rf=np.sqrt(MSE_rf)\n",
    "print(\"RMSE :\",RMSE_rf)\n",
    "#calculate MAE\n",
    "MAE_rf= mean_absolute_error(y_train, y_pred_train_r)\n",
    "print(\"MAE :\",MAE_rf)\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_rf= r2_score(y_train, y_pred_train_r)\n",
    "print(\"R2 :\",r2_rf)\n",
    "Adjusted_R2_rf=(1-(1-r2_score(y_train, y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**Looks like our r2 score value is 0.99 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict1={'Model':'Random forest regression ',\n",
    "       'MAE':round((MAE_rf),3),\n",
    "       'MSE':round((MSE_rf),3),\n",
    "       'RMSE':round((RMSE_rf),3),\n",
    "       'R2_score':round((r2_rf),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_rf ),2)}\n",
    "training_df=training_df.append(dict1,ignore_index=True)\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#calculate MSE\n",
    "MSE_rf= mean_squared_error(y_test, y_pred_test_r)\n",
    "print(\"MSE :\",MSE_rf)\n",
    "#calculate RMSE\n",
    "RMSE_rf=np.sqrt(MSE_rf)\n",
    "print(\"RMSE :\",RMSE_rf)\n",
    "#calculate MAE\n",
    "MAE_rf= mean_absolute_error(y_test, y_pred_test_r)\n",
    "print(\"MAE :\",MAE_rf)\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_rf= r2_score((y_test), (y_pred_test_r))\n",
    "print(\"R2 :\",r2_rf)\n",
    "Adjusted_R2_rf=(1-(1-r2_score((y_test), (y_pred_test_r)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_r)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**The r2_score for the test set is 0.93. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict2={'Model':'Random forest regression ',\n",
    "       'MAE':round((MAE_rf),3),\n",
    "       'MSE':round((MSE_rf),3),\n",
    "       'RMSE':round((RMSE_rf),3),\n",
    "       'R2_score':round((r2_rf),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_rf ),2)}\n",
    "test_df=test_df.append(dict2,ignore_index=True)\n",
    "\n",
    "### Heteroscadacity\n",
    "plt.scatter((y_pred_test_r),(y_test)-(y_pred_test_r))\n",
    "\n",
    "rf_model.feature_importances_\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "importance_dict = {'Feature' : list(X_train.columns),\n",
    "                   'Feature Importance' : importances}\n",
    "importance_df = pd.DataFrame(importance_dict)\n",
    "\n",
    "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n",
    "\n",
    "importance_df.sort_values(by=['Feature Importance'],ascending=False)\n",
    "\n",
    "#fit the model\n",
    "rf_model.fit(X_train,y_train)\n",
    "\n",
    "features = X_train.columns\n",
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "#Plot the figure\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.title('Feature Importance')\n",
    "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"#<font color =red> **GRADIENT BOOSTING**\"\"\"\n",
    "\n",
    "#import the packages\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# Create an instance of the GradientBoostingRegressor\n",
    "gb_model = GradientBoostingRegressor()\n",
    "gb_model.fit(X_train,y_train)\n",
    "\n",
    "# Making predictions on train and test data\n",
    "y_pred_train_g = gb_model.predict(X_train)\n",
    "y_pred_test_g = gb_model.predict(X_test)\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Model Score:\",gb_model.score(X_train,y_train))\n",
    "#calculate MSE\n",
    "MSE_gb= mean_squared_error(y_train, y_pred_train_g)\n",
    "print(\"MSE :\",MSE_gb)\n",
    "#calculate RMSE\n",
    "RMSE_gb=np.sqrt(MSE_gb)\n",
    "print(\"RMSE :\",RMSE_gb)\n",
    "#calculate MAE\n",
    "MAE_gb= mean_absolute_error(y_train, y_pred_train_g)\n",
    "print(\"MAE :\",MAE_gb)\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_gb= r2_score(y_train, y_pred_train_g)\n",
    "print(\"R2 :\",r2_gb)\n",
    "Adjusted_R2_gb = (1-(1-r2_score(y_train, y_pred_train_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**Looks like our r2 score value is 0.89 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**\"\"\"\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict1={'Model':'Gradient boosting regression ',\n",
    "       'MAE':round((MAE_gb),3),\n",
    "       'MSE':round((MSE_gb),3),\n",
    "       'RMSE':round((RMSE_gb),3),\n",
    "       'R2_score':round((r2_gb),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
    "       }\n",
    "training_df=training_df.append(dict1,ignore_index=True)\n",
    "\n",
    "#import the packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#calculate MSE\n",
    "MSE_gb= mean_squared_error(y_test, y_pred_test_g)\n",
    "print(\"MSE :\",MSE_gb)\n",
    "#calculate RMSE\n",
    "RMSE_gb=np.sqrt(MSE_gb)\n",
    "print(\"RMSE :\",RMSE_gb)\n",
    "#calculate MAE\n",
    "MAE_gb= mean_absolute_error(y_test, y_pred_test_g)\n",
    "print(\"MAE :\",MAE_gb)\n",
    "#import the packages\n",
    "from sklearn.metrics import r2_score\n",
    "#calculate r2 and adjusted r2\n",
    "r2_gb= r2_score((y_test), (y_pred_test_g))\n",
    "print(\"R2 :\",r2_gb)\n",
    "Adjusted_R2_gb = (1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "\"\"\"**The r2_score for the test set is 0.88. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict2={'Model':'Gradient boosting regression ',\n",
    "       'MAE':round((MAE_gb),3),\n",
    "       'MSE':round((MSE_gb),3),\n",
    "       'RMSE':round((RMSE_gb),3),\n",
    "       'R2_score':round((r2_gb),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
    "       }\n",
    "test_df=test_df.append(dict2,ignore_index=True)\n",
    "\n",
    "### Heteroscadacity\n",
    "plt.scatter((y_pred_test_g),(y_test)-(y_pred_test_g))\n",
    "\n",
    "gb_model.feature_importances_\n",
    "\n",
    "importances = gb_model.feature_importances_\n",
    "importance_dict = {'Feature' : list(X_train.columns),\n",
    "                   'Feature Importance' : importances}\n",
    "importance_df = pd.DataFrame(importance_dict)\n",
    "\n",
    "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n",
    "\n",
    "importance_df.head()\n",
    "\n",
    "importance_df.sort_values(by=['Feature Importance'],ascending=False)\n",
    "\n",
    "gb_model.fit(X_train,y_train)\n",
    "\n",
    "features = X_train.columns\n",
    "importances = gb_model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "#Plot the figure\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.title('Feature Importance')\n",
    "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"#<font color=red>**Hyperparameter tuning**\n",
    "\n",
    "###<font color=blue> **Provide the range of values for chosen hyperparameters**\n",
    "\"\"\"\n",
    "\n",
    "# Number of trees\n",
    "n_estimators = [50,80,100]\n",
    "# Maximum depth of trees\n",
    "max_depth = [4,6,8]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [50,100,150]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [40,50]\n",
    "# HYperparameter Grid\n",
    "param_dict = {'n_estimators' : n_estimators,\n",
    "              'max_depth' : max_depth,\n",
    "              'min_samples_split' : min_samples_split,\n",
    "              'min_samples_leaf' : min_samples_leaf}\n",
    "\n",
    "param_dict\n",
    "\n",
    "\"\"\"###<font color=green> **Importing Gradient Boosting Regressor**\"\"\"\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create an instance of the GradientBoostingRegressor\n",
    "gb_model = GradientBoostingRegressor()\n",
    "# Grid search\n",
    "gb_grid = GridSearchCV(estimator=gb_model,\n",
    "                       param_grid = param_dict,\n",
    "                       cv = 5, verbose=2)\n",
    "\n",
    "gb_grid.fit(X_train,y_train)\n",
    "\n",
    "gb_grid.best_estimator_\n",
    "\n",
    "gb_optimal_model = gb_grid.best_estimator_\n",
    "\n",
    "gb_grid.best_params_\n",
    "\n",
    "# Making predictions on train and test data\n",
    "\n",
    "y_pred_train_g_g = gb_optimal_model.predict(X_train)\n",
    "y_pred_g_g= gb_optimal_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Model Score:\",gb_optimal_model.score(X_train,y_train))\n",
    "MSE_gbh= mean_squared_error(y_train, y_pred_train_g_g)\n",
    "print(\"MSE :\",MSE_gbh)\n",
    "RMSE_gbh=np.sqrt(MSE_gbh)\n",
    "print(\"RMSE :\",RMSE_gbh)\n",
    "MAE_gbh= mean_absolute_error(y_train, y_pred_train_g_g)\n",
    "print(\"MAE :\",MAE_gbh)\n",
    "from sklearn.metrics import r2_score\n",
    "r2_gbh= r2_score(y_train, y_pred_train_g_g)\n",
    "print(\"R2 :\",r2_gbh)\n",
    "Adjusted_R2_gbh = (1-(1-r2_score(y_train, y_pred_train_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict1={'Model':'Gradient Boosting gridsearchcv ',\n",
    "       'MAE':round((MAE_gbh),3),\n",
    "       'MSE':round((MSE_gbh),3),\n",
    "       'RMSE':round((RMSE_gbh),3),\n",
    "       'R2_score':round((r2_gbh),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
    "      }\n",
    "training_df=training_df.append(dict1,ignore_index=True)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE_gbh= mean_squared_error(y_test, y_pred_g_g)\n",
    "print(\"MSE :\",MSE_gbh)\n",
    "RMSE_gbh=np.sqrt(MSE_gbh)\n",
    "print(\"RMSE :\",RMSE_gbh)\n",
    "MAE_gbh= mean_absolute_error(y_test, y_pred_g_g)\n",
    "print(\"MAE :\",MAE_gbh)\n",
    "from sklearn.metrics import r2_score\n",
    "r2_gbh= r2_score((y_test), (y_pred_g_g))\n",
    "print(\"R2 :\",r2_gbh)\n",
    "Adjusted_R2_gbh = (1-(1-r2_score(y_test, y_pred_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_g_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
    "\n",
    "# storing the test set metrics value in a dataframe for later comparison\n",
    "dict2={'Model':'Gradient Boosting gridsearchcv ',\n",
    "       'MAE':round((MAE_gbh),3),\n",
    "       'MSE':round((MSE_gbh),3),\n",
    "       'RMSE':round((RMSE_gbh),3),\n",
    "       'R2_score':round((r2_gbh),3),\n",
    "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
    "      }\n",
    "test_df=test_df.append(dict2,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "### Heteroscadacity\n",
    "plt.scatter((y_pred_g_g),(y_test)-(y_pred_g_g))\n",
    "\n",
    "gb_optimal_model.feature_importances_\n",
    "\n",
    "importances = gb_optimal_model.feature_importances_\n",
    "importance_dict = {'Feature' : list(X_train.columns),\n",
    "                   'Feature Importance' : importances}\n",
    "importance_df = pd.DataFrame(importance_dict)\n",
    "\n",
    "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n",
    "\n",
    "importance_df.head()\n",
    "\n",
    "importance_df.sort_values(by=['Feature Importance'],ascending=False)\n",
    "\n",
    "gb_model.fit(X_train,y_train)\n",
    "\n",
    "features = X_train.columns\n",
    "importances = gb_model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "#Plot the figure\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.title('Feature Importance')\n",
    "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"#<font color =red >**CONCLUSION**\n",
    "\n",
    "<font color = green >During the time of our analysis, we initially did EDA on all the features of our dataset. We first analyses  our dependent variable, 'Rented Bike Count' and also transformed it. Next we analyses  categorical variable and dropped the variable who had majority of one class, we also analyses numerical variable, found out the correlation, distribution and their relationship with the dependent variable. We also removed some numerical features who had mostly 0 values and hot encoded the categorical variables.\n",
    "\n",
    "<font color = green >Next we implemented 7 machine learning algorithms Linear Regression,lasso,ridge,elasticnet,decission tree, Random Forest and XGBoost. We did hyperparameter tuning to improve our model performance. The results of our evaluation are:\n",
    "\"\"\"\n",
    "\n",
    "# displaying the results of evaluation metric values for all models\n",
    "result=pd.concat([training_df,test_df],keys=['Training set','Test set'])\n",
    "result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529fc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf848041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8ae3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed8e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225e5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e2642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9385e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50196351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d369a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd041d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727306d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f7817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f8e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1014b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e34ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06a56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3cf36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb8236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6a86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe536a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b31f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
